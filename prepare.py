"""
Usage instructions:
    python prepare.py ch_infopath \
        -s="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/data_Ben/labeled/"
    python prepare.py ass_pet \
        -s="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/data_Ben/labeled/"
    python prepare.py prep_methodist \
        -s="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/data_Ben/masked_first/" \
        -r="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/data_Ben/labeled/" \
        -m=True \
        -c=True
    python prepare.py prep_luna
    python prepare.py get_info \
        -r="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/data_Ben/labeled"
    python prepare.py extract \
        -r="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/additional_0412/raw" \
        -s="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/additional_0412" \
        -p="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/additional_0412/checklist.xlsx" \
        -n=False
    python prepare.py convert \
        -r="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/additional_0412/raw" \
        -s="/home/cougarnet.uh.edu/pyuan2/Datasets/Methodist_incidental/additional_0412" \
        -a="labels_my-project-name_2021-04-20-11-13-39.csv" \
        -n=False
    python prepare.py update_details \
        -r="/home/cougarnet.uh.edu/pyuan2/Projects/DeepLung-3D_Lung_Nodule_Detection/Methodist_incidental/data_Ben/labeled" \
        -s="/home/cougarnet.uh.edu/pyuan2/Projects/DeepLung-3D_Lung_Nodule_Detection/Methodist_incidental"
    python prepare.py update_checklist \
        -r="/home/cougarnet.uh.edu/pyuan2/Projects/DeepLung-3D_Lung_Nodule_Detection/Methodist_incidental/data_Ben/labeled" \
        -s="/home/cougarnet.uh.edu/pyuan2/Projects/DeepLung-3D_Lung_Nodule_Detection/Methodist_incidental" \
        -p="/home/cougarnet.uh.edu/pyuan2/Projects/DeepLung-3D_Lung_Nodule_Detection/Methodist_incidental/checklist_Ben.xlsx"
"""

import os
from shutil import copyfile
import numpy as np
import scipy
import matplotlib.pyplot as plt
from scipy.ndimage.interpolation import zoom
from sklearn.cluster import KMeans
from skimage import morphology
from skimage import measure
import SimpleITK as sitk
from scipy.ndimage.morphology import binary_dilation, generate_binary_structure
from skimage.morphology import convex_hull_image
import pandas
from multiprocessing import Pool
from functools import partial
from tqdm import tqdm
from natsort import natsorted
import pandas as pd
import argparse

import warnings

config = {'train_data_path': ['LUNA16/raw_files/subset0/',
                              'LUNA16/raw_files/subset1/',
                              'LUNA16/raw_files/subset2/',
                              'LUNA16/raw_files/subset3/',
                              'LUNA16/raw_files/subset4/',
                              'LUNA16/raw_files/subset5/',
                              'LUNA16/raw_files/subset6/',
                              'LUNA16/raw_files/subset7/',
                              'LUNA16/raw_files/subset8/'],
          'val_data_path': ['LUNA16/raw_files/subset9/'],
          'test_data_path': ['LUNA16/raw_files/subset9/'],

          'train_preprocess_result_path': 'LUNA16/raw_npz/',
          # 'train_preprocess_result_path': 'LUNA16/preprocessed/',
          # contains numpy for the data and label, which is generated by prepare.py
          'val_preprocess_result_path': 'LUNA16/raw_npz/',
          # 'val_preprocess_result_path': 'LUNA16/preprocessed/',
          # make sure copy all the numpy into one folder after prepare.py
          'test_preprocess_result_path': 'LUNA16/raw_npz/',
          # 'test_preprocess_result_path': 'LUNA16/preprocessed/',

          'train_annos_path': 'LUNA16/annotations.csv',
          'val_annos_path': 'LUNA16/annotations.csv',
          'test_annos_path': 'LUNA16/annotations.csv',

          'black_list': [],

          'preprocessing_backend': 'python',

          'luna_segment': 'LUNA16/seg-lungs-LUNA16/',  # download from https://luna16.grand-challenge.org/data/
          # 'preprocess_result_path': 'LUNA16/preprocessed/',
          'preprocess_result_path': 'LUNA16/raw_npz/',
          'luna_data': 'LUNA16/raw_files/',
          'luna_label': 'LUNA16/annotations.csv'
          }

def resample(imgs, spacing, new_spacing, order=2):
    if len(imgs.shape) == 3:
        new_shape = np.round(imgs.shape * spacing / new_spacing)
        true_spacing = spacing * imgs.shape / new_shape
        resize_factor = new_shape / imgs.shape
        imgs = zoom(imgs, resize_factor, mode='nearest', order=order)
        return imgs, true_spacing
    elif len(imgs.shape) == 4:
        n = imgs.shape[-1]
        newimg = []
        for i in range(n):
            slice = imgs[:, :, :, i]
            newslice, true_spacing = resample(slice, spacing, new_spacing)
            newimg.append(newslice)
        newimg = np.transpose(np.array(newimg), [1, 2, 3, 0])
        return newimg, true_spacing
    else:
        raise ValueError('wrong shape')

def worldToVoxelCoord(worldCoord, origin, spacing):
    stretchedVoxelCoord = np.absolute(worldCoord - origin)
    voxelCoord = stretchedVoxelCoord / spacing
    return voxelCoord

def load_itk_image(filename):
    with open(filename) as f:
        contents = f.readlines()
        line = [k for k in contents if k.startswith('TransformMatrix')][0]
        transformM = np.array(line.split(' = ')[1].split(' ')).astype('float')
        transformM = np.round(transformM)
        if np.any(transformM != np.array([1, 0, 0, 0, 1, 0, 0, 0, 1])):
            isflip = True
        else:
            isflip = False

    itkimage = sitk.ReadImage(filename)
    print("read itkimage successfully")
    numpyImage = sitk.GetArrayFromImage(itkimage)

    numpyOrigin = np.array(list(reversed(itkimage.GetOrigin())))
    numpySpacing = np.array(list(reversed(itkimage.GetSpacing())))

    return numpyImage, numpyOrigin, numpySpacing, isflip

def process_mask(mask):
    convex_mask = np.copy(mask)
    for i_layer in range(convex_mask.shape[0]):
        mask1 = np.ascontiguousarray(mask[i_layer])
        if np.sum(mask1) > 0:
            mask2 = convex_hull_image(mask1)
            if np.sum(mask2) > 1.5 * np.sum(mask1):
                mask2 = mask1
        else:
            mask2 = mask1
        convex_mask[i_layer] = mask2
    struct = generate_binary_structure(3, 1)
    dilatedMask = binary_dilation(convex_mask, structure=struct, iterations=10)
    return dilatedMask

def lumTrans(img):
    lungwin = np.array([-1200., 600.])
    newimg = (img - lungwin[0]) / (lungwin[1] - lungwin[0])
    newimg[newimg < 0] = 0
    newimg[newimg > 1] = 1
    newimg = (newimg * 255).astype('uint8')
    return newimg

def binarize_per_slice(image, spacing, intensity_th=-600, sigma=1, area_th=30, eccen_th=0.99, bg_patch_size=10):
    bw = np.zeros(image.shape, dtype=bool)

    # prepare a mask, with all corner values set to nan
    image_size = image.shape[1]
    grid_axis = np.linspace(-image_size / 2 + 0.5, image_size / 2 - 0.5, image_size)
    x, y = np.meshgrid(grid_axis, grid_axis)
    d = (x ** 2 + y ** 2) ** 0.5
    nan_mask = (d < image_size / 2).astype(float)
    nan_mask[nan_mask == 0] = np.nan
    for i in range(image.shape[0]):
        # Check if corner pixels are identical, if so the slice  before Gaussian filtering
        if len(np.unique(image[i, 0:bg_patch_size, 0:bg_patch_size])) == 1:
            current_bw = scipy.ndimage.filters.gaussian_filter(np.multiply(image[i].astype('float32'), nan_mask), sigma,
                                                               truncate=2.0) < intensity_th
        else:
            current_bw = scipy.ndimage.filters.gaussian_filter(image[i].astype('float32'), sigma,
                                                               truncate=2.0) < intensity_th

        # select proper components
        label = measure.label(current_bw)
        properties = measure.regionprops(label)
        valid_label = set()
        for prop in properties:
            if prop.area * spacing[1] * spacing[2] > area_th and prop.eccentricity < eccen_th:
                valid_label.add(prop.label)
        current_bw = np.in1d(label, list(valid_label)).reshape(label.shape)
        bw[i] = current_bw

    return bw

def all_slice_analysis(bw, spacing, cut_num=0, vol_limit=[0.68, 8.2], area_th=6e3, dist_th=62):
    # in some cases, several top layers need to be removed first
    if cut_num > 0:
        bw0 = np.copy(bw)
        bw[-cut_num:] = False
    label = measure.label(bw, connectivity=1)
    # remove components access to corners
    mid = int(label.shape[2] / 2)
    bg_label = set([label[0, 0, 0], label[0, 0, -1], label[0, -1, 0], label[0, -1, -1], \
                    label[-1 - cut_num, 0, 0], label[-1 - cut_num, 0, -1], label[-1 - cut_num, -1, 0],
                    label[-1 - cut_num, -1, -1], \
                    label[0, 0, mid], label[0, -1, mid], label[-1 - cut_num, 0, mid], label[-1 - cut_num, -1, mid]])
    for l in bg_label:
        label[label == l] = 0

    # select components based on volume
    properties = measure.regionprops(label)
    for prop in properties:
        if prop.area * spacing.prod() < vol_limit[0] * 1e6 or prop.area * spacing.prod() > vol_limit[1] * 1e6:
            label[label == prop.label] = 0

    # prepare a distance map for further analysis
    x_axis = np.linspace(-label.shape[1] / 2 + 0.5, label.shape[1] / 2 - 0.5, label.shape[1]) * spacing[1]
    y_axis = np.linspace(-label.shape[2] / 2 + 0.5, label.shape[2] / 2 - 0.5, label.shape[2]) * spacing[2]
    x, y = np.meshgrid(x_axis, y_axis)
    d = (x ** 2 + y ** 2) ** 0.5
    vols = measure.regionprops(label)
    valid_label = set()
    # select components based on their area and distance to center axis on all slices
    for vol in vols:
        single_vol = label == vol.label
        slice_area = np.zeros(label.shape[0])
        min_distance = np.zeros(label.shape[0])
        for i in range(label.shape[0]):
            slice_area[i] = np.sum(single_vol[i]) * np.prod(spacing[1:3])
            min_distance[i] = np.min(single_vol[i] * d + (1 - single_vol[i]) * np.max(d))

        if np.average([min_distance[i] for i in range(label.shape[0]) if slice_area[i] > area_th]) < dist_th:
            valid_label.add(vol.label)

    bw = np.in1d(label, list(valid_label)).reshape(label.shape)

    # fill back the parts removed earlier
    if cut_num > 0:
        # bw1 is bw with removed slices, bw2 is a dilated version of bw, part of their intersection is returned as final mask
        bw1 = np.copy(bw)
        bw1[-cut_num:] = bw0[-cut_num:]
        bw2 = np.copy(bw)
        bw2 = scipy.ndimage.binary_dilation(bw2, iterations=cut_num)
        bw3 = bw1 & bw2
        label = measure.label(bw, connectivity=1)
        label3 = measure.label(bw3, connectivity=1)
        l_list = list(set(np.unique(label)) - {0})
        valid_l3 = set()
        for l in l_list:
            indices = np.nonzero(label == l)
            l3 = label3[indices[0][0], indices[1][0], indices[2][0]]
            if l3 > 0:
                valid_l3.add(l3)
        bw = np.in1d(label3, list(valid_l3)).reshape(label3.shape)

    return bw, len(valid_label)

def fill_hole(bw):
    # fill 3d holes
    label = measure.label(~bw)
    # idendify corner components
    bg_label = set([label[0, 0, 0], label[0, 0, -1], label[0, -1, 0], label[0, -1, -1], \
                    label[-1, 0, 0], label[-1, 0, -1], label[-1, -1, 0], label[-1, -1, -1]])
    bw = ~np.in1d(label, list(bg_label)).reshape(label.shape)

    return bw

def two_lung_only(bw, spacing, max_iter=22, max_ratio=4.8):
    def extract_main(bw, cover=0.95):
        for i in range(bw.shape[0]):
            current_slice = bw[i]
            label = measure.label(current_slice)
            properties = measure.regionprops(label)
            properties.sort(key=lambda x: x.area, reverse=True)
            area = [prop.area for prop in properties]
            count = 0
            sum = 0
            while sum < np.sum(area) * cover:
                sum = sum + area[count]
                count = count + 1
            filter = np.zeros(current_slice.shape, dtype=bool)
            for j in range(count):
                bb = properties[j].bbox
                filter[bb[0]:bb[2], bb[1]:bb[3]] = filter[bb[0]:bb[2], bb[1]:bb[3]] | properties[j].convex_image
            bw[i] = bw[i] & filter

        label = measure.label(bw)
        properties = measure.regionprops(label)
        properties.sort(key=lambda x: x.area, reverse=True)
        bw = label == properties[0].label

        return bw

    def fill_2d_hole(bw):
        for i in range(bw.shape[0]):
            current_slice = bw[i]
            label = measure.label(current_slice)
            properties = measure.regionprops(label)
            for prop in properties:
                bb = prop.bbox
                current_slice[bb[0]:bb[2], bb[1]:bb[3]] = current_slice[bb[0]:bb[2], bb[1]:bb[3]] | prop.filled_image
            bw[i] = current_slice

        return bw

    found_flag = False
    iter_count = 0
    bw0 = np.copy(bw)
    while not found_flag and iter_count < max_iter:
        label = measure.label(bw, connectivity=2)
        properties = measure.regionprops(label)
        properties.sort(key=lambda x: x.area, reverse=True)
        if len(properties) > 1 and properties[0].area / properties[1].area < max_ratio:
            found_flag = True
            bw1 = label == properties[0].label
            bw2 = label == properties[1].label
        else:
            bw = scipy.ndimage.binary_erosion(bw)
            iter_count = iter_count + 1

    if found_flag:
        d1 = scipy.ndimage.morphology.distance_transform_edt(bw1 == False, sampling=spacing)
        d2 = scipy.ndimage.morphology.distance_transform_edt(bw2 == False, sampling=spacing)
        bw1 = bw0 & (d1 < d2)
        bw2 = bw0 & (d1 > d2)

        bw1 = extract_main(bw1)
        bw2 = extract_main(bw2)

    else:
        bw1 = bw0
        bw2 = np.zeros(bw.shape).astype('bool')

    bw1 = fill_2d_hole(bw1)
    bw2 = fill_2d_hole(bw2)
    bw = bw1 | bw2

    return bw1, bw2, bw

def step1_python_tianchi(case_path):
    # case = load_scan(case_path)
    # case_pixels, spacing = get_pixels_hu(case)
    ''' For the mhd file reader '''
    resolution = np.array([1, 1, 1])
    sliceim, origin, spacing, isflip = load_itk_image(case_path + '.mhd')
    if isflip:
        sliceim = sliceim[:, ::-1, ::-1]
        print('flip!')
    # sliceim = lumTrans(sliceim)
    # sliceim1,_ = resample(sliceim,spacing,resolution,order=1)
    case_pixels = np.array(sliceim)

    bw = binarize_per_slice(case_pixels, spacing)
    flag = 0
    cut_num = 0
    cut_step = 2
    bw0 = np.copy(bw)
    while flag == 0 and cut_num < bw.shape[0]:
        bw = np.copy(bw0)
        bw, flag = all_slice_analysis(bw, spacing, cut_num=cut_num, vol_limit=[0.68, 7.5])
        cut_num = cut_num + cut_step

    bw = fill_hole(bw)
    bw1, bw2, bw = two_lung_only(bw, spacing)
    return case_pixels, bw1, bw2, spacing, origin, isflip

def savenpy(id, annos, filelist, data_path, prep_folder):
    resolution = np.array([1, 1, 1])
    name = filelist[id]
    im, m1, m2, spacing, origin, isflip = step1_python_tianchi(os.path.join(data_path, name))
    missingmask = False
    if os.path.exists(os.path.join(prep_folder, name + '_clean.npy')) and \
            os.path.exists(os.path.join(prep_folder, name + '_originbox.npy')) and \
            os.path.exists(os.path.join(prep_folder, name + '_spacing.npy')) and \
            os.path.exists(os.path.join(prep_folder, name + '_origin.npy')) and \
            os.path.exists(os.path.join(prep_folder, name + '_label.npy')):
        if not isflip:
            print('skip', name)
            return
        else:
            missingmask = True

    print('process', name)
    label = annos[annos[:, 0] == name]
    # label = label.astype('float')
    label = label[:, [3, 1, 2, 4]].astype('float')  # z, y, x, d

    Mask = m1 + m2

    newshape = np.round(np.array(Mask.shape) * spacing / resolution)
    xx, yy, zz = np.where(Mask)
    if xx.size == 0 or yy.size == 0 or zz.size == 0:
        print(name)
        assert 1 == 0

    box = np.array([[np.min(xx), np.max(xx)], [np.min(yy), np.max(yy)], [np.min(zz), np.max(zz)]])
    box = box * np.expand_dims(spacing, 1) / np.expand_dims(resolution, 1)
    box = np.floor(box).astype('int')
    margin = 5
    extendbox = np.vstack(
        [np.max([[0, 0, 0], box[:, 0] - margin], 0), np.min([newshape, box[:, 1] + 2 * margin], axis=0).T]).T
    extendbox = extendbox.astype('int')
    if extendbox[0, 0] == extendbox[0, 1] or extendbox[1, 0] == extendbox[1, 1] or extendbox[2, 0] == extendbox[2, 1]:
        print(name)
        assert 1 == 0

    convex_mask = m1
    dm1 = process_mask(m1)
    dm2 = process_mask(m2)
    dilatedMask = dm1 + dm2
    Mask = m1 + m2
    if missingmask:
        np.save(os.path.join(prep_folder, name + '_mask.npy'), Mask)
        print('skip', name)
        return
    extramask = dilatedMask - Mask
    bone_thresh = 210
    pad_value = 170
    im[np.isnan(im)] = -2000
    sliceim = lumTrans(im)
    sliceim = sliceim * dilatedMask + pad_value * (1 - dilatedMask).astype('uint8')
    bones = sliceim * extramask > bone_thresh
    sliceim[bones] = pad_value
    sliceim1, _ = resample(sliceim, spacing, resolution, order=1)
    sliceim2 = sliceim1[extendbox[0, 0]:extendbox[0, 1],
               extendbox[1, 0]:extendbox[1, 1],
               extendbox[2, 0]:extendbox[2, 1]]
    sliceim = sliceim2[np.newaxis, ...]
    np.save(os.path.join(prep_folder, name + '_clean.npy'), sliceim)
    np.save(os.path.join(prep_folder, name + '_originbox.npy'), extendbox)
    np.save(os.path.join(prep_folder, name + '_spacing.npy'), spacing)
    np.save(os.path.join(prep_folder, name + '_origin.npy'), origin)
    print(im.shape, '_clean', sliceim.shape, '_originbox', extendbox.shape, '_space', spacing, '_origin', origin)

    this_annos = np.copy(annos[annos[:, 0] == name])
    label = []
    print('label', this_annos.shape, name)
    if len(this_annos) > 0:

        for c in this_annos:
            pos = worldToVoxelCoord(c[1:4][::-1], origin=origin, spacing=spacing)
            if isflip:
                pos[1:] = Mask.shape[1:3] - pos[1:]
            label.append(np.concatenate([pos, [c[4] / spacing[1]]]))

    label = np.array(label)
    if len(label) == 0:
        label2 = np.array([[0, 0, 0, 0]])
    else:
        label2 = np.copy(label).T
        label2[:3] = label2[:3] * np.expand_dims(spacing, 1) / np.expand_dims(resolution, 1)
        label2[3] = label2[3] * spacing[1] / resolution[1]
        label2[:3] = label2[:3] - np.expand_dims(extendbox[:, 0], 1)
        label2 = label2[:4].T
    np.save(os.path.join(prep_folder, name + '_label.npy'), label2)
    print(name)

def full_prep(train=True, val=True, test=True):
    warnings.filterwarnings("ignore")
    # preprocess_result_path = './prep_result'
    train_prep_folder = config['train_preprocess_result_path']
    val_prep_folder = config['val_preprocess_result_path']
    test_prep_folder = config['test_preprocess_result_path']

    train_data_path = config['train_data_path']
    val_data_path = config['val_data_path']
    test_data_path = config['test_data_path']

    finished_flag = '.flag_preptianchi'

    if not os.path.exists(finished_flag):
        trainlabelfiles = config['train_annos_path']
        vallabelfiles = config['val_annos_path']
        testlabelfiles = config['test_annos_path']

        traincontent = np.array(pandas.read_csv(trainlabelfiles))
        traincontent = traincontent[traincontent[:, 0] != np.nan]
        trainalllabel = traincontent[1:, :]  # filename, x, y, z, d
        trainfilelist = []
        for f in os.listdir(config['train_data_path']):
            if f.endswith('.mhd'):
                if f[:-4] in config['black_list']:
                    continue
                trainfilelist.append(f[:-4])

        valcontent = np.array(pandas.read_csv(vallabelfiles))
        valcontent = valcontent[valcontent[:, 0] != np.nan]
        valalllabel = valcontent[1:, :]  # filename, x, y, z, d
        valfilelist = []
        for f in os.listdir(config['val_data_path']):
            if f.endswith('.mhd'):
                if f[:-4] in config['black_list']:
                    continue
                valfilelist.append(f[:-4])

        testcontent = np.array(pandas.read_csv(testlabelfiles))
        testcontent = testcontent[testcontent[:, 0] != np.nan]
        testalllabel = testcontent[1:, :]  # filename, x, y, z, d
        testfilelist = []
        for f in os.listdir(config['test_data_path']):
            if f.endswith('.mhd'):
                if f[:-4] in config['black_list']:
                    continue
                testfilelist.append(f[:-4])

        if not os.path.exists(train_prep_folder):
            os.mkdir(train_prep_folder)
        if not os.path.exists(val_prep_folder):
            os.mkdir(val_prep_folder)
        if not os.path.exists(test_prep_folder):
            os.mkdir(test_prep_folder)
        # eng.addpath('preprocessing/',nargout=0)
        if train:
            print('starting train preprocessing')
            pool = Pool(10)
            partial_savenpy = partial(savenpy, annos=trainalllabel, filelist=trainfilelist, data_path=train_data_path,
                                      prep_folder=train_prep_folder)
            N = len(trainfilelist)
            savenpy(1)
            _ = pool.map(partial_savenpy, list(range(N)))
            print('end train preprocessing')
        if val:
            print('starting val preprocessing')
            partial_savenpy = partial(savenpy, annos=valalllabel, filelist=valfilelist, data_path=val_data_path,
                                      prep_folder=val_prep_folder)
            N = len(valfilelist)
            savenpy(1)
            _ = pool.map(partial_savenpy, list(range(N)))
            print('end val preprocessing')
        if test:
            print('starting test preprocessing')
            partial_savenpy = partial(savenpy, annos=testalllabel, filelist=testfilelist, data_path=test_data_path,
                                      prep_folder=test_prep_folder)
            N = len(testfilelist)
            savenpy(1)
            _ = pool.map(partial_savenpy, list(range(N)))
            pool.close()
            pool.join()
            print('end test preprocessing')
    f = open(finished_flag, "w+")

def splitvaltestcsv():
    testfiles = []
    for f in os.listdir(config['test_data_path']):
        if f.endswith('.mhd'):
            testfiles.append(f[:-4])
    valcsvlines = []
    testcsvlines = []
    import csv
    valf = open(config['val_annos_path'], 'r')
    valfcsv = csv.reader(valf)
    for line in valfcsv:
        if line[0] in testfiles:
            testcsvlines.append(line)
        else:
            valcsvlines.append(line)
    valf.close()
    testf = open(config['test_annos_path'] + 'annotations.csv', 'w')
    testfcsv = csv.writer(testf)
    for line in testcsvlines:
        testfcsv.writerow(line)
    testf.close()
    valf = open(config['val_annos_path'], 'w')
    valfcsv = csv.writer(valf)
    for line in valcsvlines:
        valfcsv.writerow(line)
    valf.close()

def savenpy_luna_raw(id, annos, filelist, luna_segment, luna_data, savepath):
    islabel = True
    isClean = True
    resolution = np.array([1, 1, 1])
    #     resolution = np.array([2,2,2])
    name = filelist[id]

    sliceim, origin, spacing, isflip = load_itk_image(os.path.join(luna_data, name + '.mhd'))
    ori_shape = sliceim.shape
    # Mask,origin,spacing,isflip = load_itk_image(os.path.join(luna_segment,name+'.mhd'))
    # if isflip:
    #     Mask = Mask[:,::-1,::-1]
    # newshape = np.round(np.array(Mask.shape)*spacing/resolution).astype('int')
    # m1 = Mask==3
    # m2 = Mask==4
    # Mask = m1+m2
    #
    # xx,yy,zz= np.where(Mask)
    # box = np.array([[np.min(xx),np.max(xx)],[np.min(yy),np.max(yy)],[np.min(zz),np.max(zz)]])
    # box = box*np.expand_dims(spacing,1)/np.expand_dims(resolution,1)
    # box = np.floor(box).astype('int')
    # margin = 5
    # extendbox = np.vstack([np.max([[0,0,0],box[:,0]-margin],0),np.min([newshape,box[:,1]+2*margin],axis=0).T]).T

    if isClean:
        # convex_mask = m1
        # dm1 = process_mask(m1)
        # dm2 = process_mask(m2)
        # dilatedMask = dm1+dm2
        # Mask = m1+m2
        #
        # extramask = dilatedMask ^ Mask
        # bone_thresh = 210
        # pad_value = 170

        if isflip:
            sliceim = sliceim[:, ::-1, ::-1]
            print('flip!')
        sliceim1, _ = resample(sliceim, spacing, resolution, order=1)

        sliceim1 = lumTrans(sliceim1)
        # sliceim = sliceim*dilatedMask+pad_value*(1-dilatedMask).astype('uint8')
        # bones = (sliceim*extramask)>bone_thresh
        # sliceim[bones] = pad_value
        #
        # sliceim1,_ = resample(sliceim,spacing,resolution,order=1)
        # sliceim2 = sliceim1[extendbox[0,0]:extendbox[0,1],
        #             extendbox[1,0]:extendbox[1,1],
        #             extendbox[2,0]:extendbox[2,1]]
        sliceim = sliceim1[np.newaxis, ...]
        np.savez_compressed(os.path.join(savepath, name + '_image.npz'), image=sliceim)
        # np.save(os.path.join(savepath, name+'_spacing.npy'), spacing)
        # np.save(os.path.join(savepath, name+'_extendbox.npy'), extendbox)
        # np.save(os.path.join(savepath, name+'_origin.npy'), origin)
        # np.save(os.path.join(savepath, name+'_mask.npy'), Mask)

    if islabel:
        this_annos = np.copy(annos[annos[:, 0] == (name)])
        label = []
        if len(this_annos) > 0:

            for c in this_annos:
                pos = worldToVoxelCoord(c[1:4][::-1], origin=origin, spacing=spacing)
                if isflip:
                    # pos[1:] = Mask.shape[1:3]-pos[1:]
                    pos[1:] = ori_shape[1:3] - pos[1:]
                label.append(np.concatenate([pos, [c[4] / spacing[1]]]))

        label = np.array(label)
        if len(label) == 0:
            label2 = np.array([[0, 0, 0, 0]])
        else:
            label2 = np.copy(label).T
            label2[:3] = label2[:3] * np.expand_dims(spacing, 1) / np.expand_dims(resolution, 1)
            label2[3] = label2[3] * spacing[1] / resolution[1]
            # label2[:3] = label2[:3]-np.expand_dims(extendbox[:,0],1)
            label2 = label2[:4].T
        # np.save(os.path.join(savepath,name+'_label.npy'), label2)
        # np.savez_compressed(os.path.join(savepath, name + '_label.npz'), label=label2)

    print(name)

def savenpy_luna(id, annos, filelist, luna_segment, luna_data, savepath):
    islabel = True
    isClean = True
    resolution = np.array([1, 1, 1])
    #     resolution = np.array([2,2,2])
    name = filelist[id]

    sliceim, origin, spacing, isflip = load_itk_image(os.path.join(luna_data, name + '.mhd'))

    Mask, origin, spacing, isflip = load_itk_image(os.path.join(luna_segment, name + '.mhd'))
    if isflip:
        Mask = Mask[:, ::-1, ::-1]
    newshape = np.round(np.array(Mask.shape) * spacing / resolution).astype('int')
    m1 = Mask == 3
    m2 = Mask == 4
    Mask = m1 + m2

    xx, yy, zz = np.where(Mask)
    box = np.array([[np.min(xx), np.max(xx)], [np.min(yy), np.max(yy)], [np.min(zz), np.max(zz)]])
    box = box * np.expand_dims(spacing, 1) / np.expand_dims(resolution, 1)
    box = np.floor(box).astype('int')
    margin = 5
    extendbox = np.vstack(
        [np.max([[0, 0, 0], box[:, 0] - margin], 0), np.min([newshape, box[:, 1] + 2 * margin], axis=0).T]).T

    this_annos = np.copy(annos[annos[:, 0] == (name)])

    if isClean:
        convex_mask = m1
        dm1 = process_mask(m1)
        dm2 = process_mask(m2)
        dilatedMask = dm1 + dm2
        Mask = m1 + m2

        extramask = dilatedMask ^ Mask
        bone_thresh = 210
        pad_value = 170

        if isflip:
            sliceim = sliceim[:, ::-1, ::-1]
            print('flip!')
        sliceim = lumTrans(sliceim)
        sliceim = sliceim * dilatedMask + pad_value * (1 - dilatedMask).astype('uint8')
        bones = (sliceim * extramask) > bone_thresh
        sliceim[bones] = pad_value

        sliceim1, _ = resample(sliceim, spacing, resolution, order=1)
        sliceim2 = sliceim1[extendbox[0, 0]:extendbox[0, 1],
                   extendbox[1, 0]:extendbox[1, 1],
                   extendbox[2, 0]:extendbox[2, 1]]
        sliceim = sliceim2[np.newaxis, ...]
        # np.save(os.path.join(savepath, name+'_clean.npy'), sliceim)
        # np.save(os.path.join(savepath, name+'_spacing.npy'), spacing)
        # np.save(os.path.join(savepath, name+'_extendbox.npy'), extendbox)
        # np.save(os.path.join(savepath, name+'_origin.npy'), origin)
        # np.save(os.path.join(savepath, name+'_mask.npy'), Mask)

    if islabel:
        this_annos = np.copy(annos[annos[:, 0] == (name)])
        label = []
        if len(this_annos) > 0:

            for c in this_annos:
                pos = worldToVoxelCoord(c[1:4][::-1], origin=origin, spacing=spacing)
                if isflip:
                    pos[1:] = Mask.shape[1:3] - pos[1:]
                label.append(np.concatenate([pos, [c[4] / spacing[1]]]))

        label = np.array(label)
        if len(label) == 0:
            label2 = np.array([[0, 0, 0, 0]])
        else:
            label2 = np.copy(label).T
            label2[:3] = label2[:3] * np.expand_dims(spacing, 1) / np.expand_dims(resolution, 1)
            label2[3] = label2[3] * spacing[1] / resolution[1]
            label2[:3] = label2[:3] - np.expand_dims(extendbox[:, 0], 1)
            label2 = label2[:4].T
        # np.save(os.path.join(savepath,name+'_label.npy'), label2)

    print(name)

def preprocess_luna():
    luna_segment = config['luna_segment']
    savepath = config['preprocess_result_path']
    luna_data = config['luna_data']
    luna_label = config['luna_label']
    finished_flag = '.flag_preprocessluna_______'
    print('starting preprocessing luna')
    if not os.path.exists(finished_flag):
        annos = np.array(pandas.read_csv(luna_label))
        # pool = Pool()
        if not os.path.exists(savepath):
            os.mkdir(savepath)
        for setidx in range(0, 1):
            print('process subset', setidx)
            filelist = [f.split('.mhd')[0] for f in os.listdir(luna_data + 'subset' + str(setidx)) if
                        f.endswith('.mhd')]
            # filelist = ["1.3.6.1.4.1.14519.5.2.1.6279.6001.105756658031515062000744821260"]
            if not os.path.exists(savepath + 'subset' + str(setidx)):
                os.mkdir(savepath + 'subset' + str(setidx))
            # partial_savenpy_luna = partial(savenpy_luna, annos=annos, filelist=filelist,
            #                            luna_segment=luna_segment, luna_data=luna_data+'subset'+str(setidx)+'/',
            #                            savepath=savepath+'subset'+str(setidx)+'/')
            N = len(filelist)
            for i in tqdm(range(N)):
                _ = savenpy_luna_raw(i, annos, filelist=filelist,
                                     luna_segment=luna_segment, luna_data=luna_data + 'subset' + str(setidx) + '/',
                                     savepath=savepath + 'subset' + str(setidx) + '/')
            # savenpy(1)
        #     _=pool.map(partial_savenpy_luna,list(range(N)))
        # pool.close()
        # pool.join()

    print('end preprocessing luna')
    f = open(finished_flag, "w+")

def change_root_info(dst_dir):
    file = os.path.join(dst_dir, "CTinfo.npz")
    infos = np.load(file, allow_pickle=True)["info"]
    s = infos[0]["imagePath"].find("Lung_patient")
    if s == -1:
        get_infos_from_npz(dst_dir)
        file = os.path.join(dst_dir, "CTinfo.npz")
        infos = np.load(file, allow_pickle=True)["info"]
    for info in infos:
        s = info["imagePath"].find("Lung_patient")
        subPath = info["imagePath"][s:].replace("\\", "/")
        subPathClean = subPath.replace(".npz", "_clean.npz")
        if not os.path.exists(os.path.join(dst_dir, subPath)) and \
            not os.path.exists(os.path.join(dst_dir, subPathClean)):
            subPathList = subPath.split("/")
            subPathList[0] = subPathList[0].rsplit("_", 1)[0]
            # if not os.path.exists(os.path.exists(os.path.join(dst_dir, "/".join(subPathList)))):
            #     subPath = subPath.replace(".npz", "_extendbox.npz")
            # # subPath = "/".join(subPathList)
            #     assert os.path.exists(os.path.join(dst_dir, subPath))
            subPath = "/".join(subPathList)
            assert os.path.exists(os.path.join(dst_dir, subPath))
        info["imagePath"] = os.path.join(dst_dir, subPath)
    print(infos)

    import shutil
    shutil.move(file, os.path.join(dst_dir, "CTinfo_old.npz"))
    np.savez_compressed(file, info=infos)
    print("Save all scan infos to {:s}".format(file))

def make_lungmask(img, display=False):
    raw_img = np.copy(img)
    row_size = img.shape[0]
    col_size = img.shape[1]

    mean = np.mean(img)
    std = np.std(img)
    if std == 0:
        return np.zeros_like(img), np.zeros_like(img, dtype=np.int8)

    img = img - mean
    img = img / std
    # Find the average pixel value near the lungs
    # to renormalize washed out images
    # middle = img[int(col_size / 5):int(col_size / 5 * 4), int(row_size / 5):int(row_size / 5 * 4)]
    middle = img[0:col_size, 0:row_size]
    mean = np.mean(middle)
    max = np.max(img)
    min = np.min(img)
    # To improve threshold finding, I'm moving the
    # underflow and overflow on the pixel spectrum
    img[img == max] = mean
    img[img == min] = mean
    #
    # Using Kmeans to separate foreground (soft tissue / bone) and background (lung/air)
    #
    kmeans = KMeans(n_clusters=2).fit(np.reshape(middle, [np.prod(middle.shape), 1]))
    centers = sorted(kmeans.cluster_centers_.flatten())
    threshold = np.mean(centers)
    thresh_img = np.where(img < threshold, 1.0, 0.0)  # threshold the image

    # First erode away the finer elements, then dilate to include some of the pixels surrounding the lung.
    # We don't want to accidentally clip the lung.

    eroded = morphology.erosion(thresh_img, np.ones([5, 5]))
    dilation = morphology.dilation(eroded, np.ones([8, 8]))

    labels = measure.label(dilation)  # Different labels are displayed in different colors
    label_vals = np.unique(labels)
    regions = measure.regionprops(labels)
    good_labels = []
    for prop in regions:
        B = prop.bbox
        if B[2] - B[0] < row_size / 10 * 9 and B[3] - B[1] < col_size / 10 * 9 and B[0] > row_size / 10 and B[
            2] < col_size / 10 * 9:
            good_labels.append(prop.label)
    mask = np.ndarray([row_size, col_size], dtype=np.int8)
    mask[:] = 0  # mask = np.zeros([row_size, col_size], dtype=np.int8)

    #
    #  After just the lungs are left, we do another large dilation
    #  in order to fill in and out the lung mask
    #
    for N in good_labels:
        mask = mask + np.where(labels == N, 1, 0)
    mask = morphology.dilation(mask, np.ones([12, 12]))  # one last dilation

    if (display):
        fig, ax = plt.subplots(3, 2, figsize=[12, 12])
        ax[0, 0].set_title("Original")
        ax[0, 0].imshow(img, cmap='gray')
        ax[0, 0].axis('off')
        ax[0, 1].set_title("Threshold")
        ax[0, 1].imshow(thresh_img, cmap='gray')
        ax[0, 1].axis('off')
        ax[1, 0].set_title("After Erosion and Dilation")
        ax[1, 0].imshow(dilation, cmap='gray')
        ax[1, 0].axis('off')
        ax[1, 1].set_title("Color Labels")
        ax[1, 1].imshow(labels)
        ax[1, 1].axis('off')
        ax[2, 0].set_title("Final Mask")
        ax[2, 0].imshow(mask, cmap='gray')
        ax[2, 0].axis('off')
        ax[2, 1].set_title("Apply Mask on Original")
        ax[2, 1].imshow(mask * img, cmap='gray')
        ax[2, 1].axis('off')

        plt.show()
    return mask * raw_img, mask

def mask_scan(images):
    masked_images = []
    masks = []
    for img in images:

        masked_image, mask = make_lungmask(img)
        masked_images.append(masked_image)
        masks.append(mask)
    masked_images = np.stack(masked_images)
    masks = np.stack(masks)
    return masked_images, masks


def prepare_masked_images(root_dir, save_dir):
    info_path = os.path.join(root_dir, "CTinfo.npz")
    infos = np.load(info_path, allow_pickle=True)["info"]
    for info in tqdm(infos):
        s = info["imagePath"].find("Lung_patient")
        save_path = os.path.join(save_dir, info["imagePath"][s:].replace("\\", "/"))
        load_path = os.path.join(root_dir, info["imagePath"][s:].replace("\\", "/"))
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        imgs = np.load(load_path, allow_pickle=True)["image"]
        imgs = mask_scan(imgs)
        imgs = lumTrans(imgs)

        info["imagePath"] = save_path
        np.savez_compressed(save_path, image=imgs, info=info)
        print("Save masked images to {:s}".format(save_path))

    new_info_path = os.path.join(save_dir, "CTinfo.npz")
    np.savez_compressed(new_info_path, info=infos)
    print("Save all scan infos to {:s}".format(new_info_path))

def prepare_masked_cropped_images(root_dir, save_dir):
    info_path = os.path.join(root_dir, "CTinfo.npz")
    infos = np.load(info_path, allow_pickle=True)["info"]
    for info in tqdm(infos):
        s = info["imagePath"].find("Lung_patient")
        save_path = os.path.join(save_dir, info["imagePath"][s:].replace("\\", "/"))
        load_path = os.path.join(root_dir, info["imagePath"][s:].replace("\\", "/"))
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        savepath = os.path.dirname(save_path)
        name = os.path.basename(save_path).strip(".npz")
        if os.path.exists(os.path.join(savepath, name + '_extendbox.npz')):
            continue

        imgs = np.load(load_path, allow_pickle=True)["image"]
        imgs, masks = mask_scan(imgs)
        imgs = lumTrans(imgs)

        zz, yy, xx = np.where(masks)
        box = np.array([[np.min(zz), np.max(zz)], [np.min(yy), np.max(yy)], [np.min(xx), np.max(xx)]])
        box = np.floor(box).astype('int')
        margin = 5
        extendbox = np.vstack([np.max([[0, 0, 0], box[:, 0] - margin], 0),
                               np.min([masks.shape, box[:, 1] + 2 * margin], axis=0).T]).T
        sliceim = imgs[extendbox[0, 0]:extendbox[0, 1],
                       extendbox[1, 0]:extendbox[1, 1],
                       extendbox[2, 0]:extendbox[2, 1]]
        sliceim = sliceim[np.newaxis, ...]

        # savepath = os.path.dirname(save_path)
        # name = os.path.basename(save_path).strip(".npz")
        # spacing = np.array([1, 1, 1])
        # origin = np.array([0, 0, 0])
        np.savez_compressed(os.path.join(savepath, name+'_clean.npz'), image=sliceim, info=info)
        # np.save(os.path.join(savepath, name+'_spacing.npy'), spacing)
        np.savez_compressed(os.path.join(savepath, name+'_extendbox.npz'), extendbox=extendbox)
        # np.save(os.path.join(savepath, name+'_origin.npy'), origin)
        np.savez_compressed(os.path.join(savepath, name+'_mask.npz'), masks=masks)

    new_info_path = os.path.join(save_dir, "CTinfo.npz")
    np.savez_compressed(new_info_path, info=infos)
    print("Save all scan infos to {:s}".format(new_info_path))
    change_root_info(save_dir)
    pos_label_file = "pos_labels_norm.csv"
    copyfile(os.path.join(root_dir, pos_label_file), os.path.join(save_dir, pos_label_file))

def assign_PET_label(dst_dir):

    PET_series = ["CT SLICES 50cm DFOV", "CTAC", "CT SLICES 50cm", "CT SLICES 50 CM", "Lung/Bone+ 50cm"]
    file = os.path.join(dst_dir, "CTinfo.npz")
    infos = np.load(file, allow_pickle=True)["info"]
    # D = {}
    # for i, a in enumerate(infos):
    #     if a["series"] not in D:
    #         D[a["series"]] = []
    #     D[a["series"]].append(i)

    for i, info in tqdm(enumerate(infos)):
        series = info["series"]
        if "PET" in info and (info["PET"] == "Y" or info["PET"] == "N"):
            continue
        if series in PET_series:
            info["PET"] = "Y"
            try:
                shape = np.load(info['imagePath'])["image"].shape
            except FileNotFoundError:
                shape = np.load(info['imagePath'].replace(".npz", "_clean.npz"))["image"].shape
            num_slices = shape[0] if len(shape) == 3 else shape[1]
            if not num_slices >= 500:
                print("index {:}, series is {:}, shape is {:}".format(i, series, shape))
        else:
            info["PET"] = "N"
            try:
                shape = np.load(info['imagePath'])["image"].shape
            except FileNotFoundError:
                shape = np.load(info['imagePath'].replace(".npz", "_clean.npz"))["image"].shape
            num_slices = shape[0] if len(shape) == 3 else shape[1]
            if not num_slices < 500:
                print("index {:}, series is {:}, shape is {:}".format(i, series, shape))
        assert len(shape) == 3, "index {:}, series is {:}, shape is {:}".format(i, series, shape)
    print(infos)

    import shutil
    shutil.move(file, os.path.join(dst_dir, "CTinfo_old.npz"))
    np.savez_compressed(file, info=infos)
    print("Save all scan infos to {:s}".format(file))

def resample_pos(label, thickness, spacing, new_spacing=[1, 1, 1], imgshape=None):
    """
    :param label: (x, y, z, d) in original resolution
    :param thickness: float z
    :param spacing: original resolution, list [y, x]
    :param new_spacing: new resolution, list [z, y, x]
    :param imgshape: tuple, (#z, #y, #x)
    :return:
    """
    label = label.astype(np.float)
    spacing = map(float, ([thickness] + list(spacing)))
    spacing = np.array(list(spacing))
    resize_factor = spacing / new_spacing
    # new_shape = np.round(imgshape * resize_factor)
    orishape = np.round(imgshape / resize_factor)
    resize_factor = (np.array(imgshape) - 1) / (orishape - 1)
    resize_factor = resize_factor[::-1]
    label[:3] = np.round(label[:3] * resize_factor).astype(np.int)
    label[3] = label[3] * resize_factor[1]

    return label

def resample_pos_back(label, thickness, spacing, new_spacing=[1, 1, 1], imgshape=None):
    """
    :param label: (x, y, z, d) in normalized resolution
    :param thickness: float z
    :param spacing: original resolution, list [y, x]
    :param new_spacing: new resolution, list [z, y, x]
    :param imgshape: tuple, (#z, #y, #x)
    :return:
    """
    label = label.astype(np.float)
    spacing = map(float, ([thickness] + list(spacing)))
    spacing = np.array(list(spacing))
    resize_factor = spacing / new_spacing
    # new_shape = np.round(imgshape * resize_factor)
    orishape = np.round(imgshape / resize_factor)
    resize_factor = (np.array(imgshape) - 1) / (orishape - 1)
    resize_factor = resize_factor[::-1]
    label[:3] = np.round(label[:3] / resize_factor).astype(np.int)
    label[3] = label[3] / resize_factor[1]

    return label


def remove_duplicate(imageInfo, exclude=[]):
    for i, info in enumerate(imageInfo):
        if info["date"] == "":
            info["date"] = info["imagePath"].strip(".npz").split("-")[-1]

    identifier_set = ["{:}-{:}".format(info["patientID"], info["date"]) for info in imageInfo]
    remove_ids = []
    from collections import Counter
    cnt = Counter(identifier_set)
    for k, v in cnt.items():
        if k in exclude:
            indices = [i for i, x in enumerate(identifier_set) if x == k]
            remove_ids.append(*indices)
        elif v > 1:
            indices = [i for i, x in enumerate(identifier_set) if x == k]
            remove_ids.append(*indices[:-1])
    imageInfo = np.delete(imageInfo, remove_ids)
    return imageInfo

# from show_results import plot_bbox
# image = np.load(info["imagePath"])["image"]
# thickness, spacing = info["sliceThickness"], info["pixelSpacing"]
# pos = np.array([202,	337.5,	123,	49.04079934])
# pos = resample_pos(pos, thickness, spacing, imgshape=image.shape)
# pos = pos[[2, 1, 0, 3]]
# plot_bbox(None, image, pos)


def show_image(info, z, new_thickness=1):
    image = np.load(info["imagePath"])["image"]
    thickness, spacing = int(info["sliceThickness"]), info["pixelSpacing"]
    factor = thickness / new_thickness
    ori_shape = np.round(image.shape[0] / factor)
    factor = (image.shape[0] - 1) / (ori_shape - 1)
    new_z = np.round(z * factor).astype(np.int)
    plt.imshow(image[new_z], cmap="gray")
    plt.show()


def check_label_existance(root_dir, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    from PIL import Image
    exclude = ["001030196-20121205", "005520101-20130316", "009453325-20130820", "034276428-20131212",
               "036568905-20150714", "038654273-20160324", "011389806-20160907", "015995871-20160929",
               "052393550-20161208", "033204314-20170207", "017478009-20170616", "027456904-20180209",
               "041293960-20170227", "000033167-20131213", "022528020-20180525", "025432105-20180730",
               "000361956-20180625"]
    pos_df = pd.read_csv(os.path.join(root_dir, "pos_labels_norm.csv"), dtype={"date": str})
    checklist_df = pd.read_excel(os.path.join(root_dir, "checklist.xlsx"), skiprows=1, dtype={"date": str})
    imageInfo = np.load(os.path.join(root_dir, "CTinfo.npz"), allow_pickle=True)["info"]
    imageInfo = remove_duplicate(imageInfo, exclude)
    no_pos = []
    for info in tqdm(imageInfo):
        # imgshape = np.load(info["imagePath"])["image"].shape

        pstr = info["pstr"]
        dstr = info["date"]
        existId = (pos_df["patient"] == pstr) & (pos_df["date"] == dstr)
        pos = pos_df[existId][["x", "y", "z", "d"]].values
        if len(pos) == 0:
            no_pos.append(info)
            save_name = "{:s}_{:s}_{:s}".format(info["pstr"], info["patientID"], info["date"])
            save_name += "_PET" if info["PET"] == "Y" else ""
            img = np.load(info["imagePath"])["image"]
            img = lumTrans(img)
            thickness = info["sliceThickness"]

            matchId = (checklist_df["Patient\n Index"] == pstr) & (checklist_df["date"] == dstr)
            assert matchId.sum() > 0, "no matches, pstr {:}, dstr {:}".format(pstr, dstr)
            z = checklist_df[matchId]["z"].values
            nodule_idx = checklist_df[matchId]["nodule\nIndex"].values
            # get norm_z
            factor = thickness / 1
            ori_shape = np.round(img.shape[0] / factor)
            factor = (img.shape[0] - 1) / (ori_shape - 1)
            for zi, ni in zip(z, nodule_idx):
                zi = zi - 1
                norm_z = np.round(zi * factor).astype(np.int)
                c_img = img[norm_z]
                save_path = os.path.join(save_dir, save_name + "_no{:d}_z{:d}.png".format(ni, norm_z))
                PILimg = Image.fromarray(c_img)
                PILimg.save(save_path)

    no_pos = natsorted(np.array(no_pos))
    print(np.array(no_pos))

def gt_labels_normalized2raw(root_dir, save_dir):
    """
    :param root_dir: where CTinfo.npz locates
    :param save_dir: where pos_labels_raw.csv locates
    :return: None; save to pos_labels_norm.csv
    """
    pos_df = pd.read_csv(os.path.join(save_dir, "pos_labels_norm.csv"), dtype={"date": str})
    change_root_info(root_dir)
    imageInfo = np.load(os.path.join(root_dir, "CTinfo.npz"), allow_pickle=True)["info"]
    imageInfo = remove_duplicate(imageInfo)
    drop_rows = []
    for i in tqdm(range(len(pos_df))):
        pos_info = pos_df.iloc[i]
        pstr = pos_info["patient"]
        dstr = pos_info["date"]
        ids = [id for id in range(len(imageInfo)) if imageInfo[id]["pstr"] == pstr and imageInfo[id]["date"] == dstr]
        assert len(ids) <= 1, "{:} \n {:}".format(ids, pos_info)
        if len(ids) == 0:
            print(pos_info["patient"])
            drop_rows.append(pos_df.index[i])
            continue
        imageId = ids[0]
        imagePath = imageInfo[imageId]["imagePath"]
        thickness = imageInfo[imageId]["sliceThickness"]
        spacing = imageInfo[imageId]["pixelSpacing"]
        # resolution = np.array(list(map(float, ([thickness] + list(spacing)))))
        # new_resolution = np.array([1, 1, 1])
        norm_pos = pos_info[["x", "y", "z", "d"]].values.astype(np.float)
        image = np.load(imagePath, allow_pickle=True)["image"]
        raw_pos = resample_pos_back(norm_pos, thickness, spacing, new_spacing=[1, 1, 1], imgshape=image.shape)
        raw_pos[2] = raw_pos[2] + 1
        pos_df.loc[pos_df.index[i], ["x", "y", "z", "d"]] = raw_pos
    pos_df = pos_df.drop(index=drop_rows)
    pos_df.to_csv(os.path.join(save_dir, "pos_labels_raw.csv"), index=False)
    print("save to {:s}".format(os.path.join(save_dir, "pos_labels_raw.csv")))



def gt_labels_raw2normalized(root_dir, save_dir):
    """
    :param root_dir: where CTinfo.npz locates
    :param save_dir: where pos_labels_raw.csv locates
    :return: None; save to pos_labels_norm.csv
    """
    pos_df = pd.read_csv(os.path.join(save_dir, "pos_labels_raw.csv"), dtype={"date": str})
    change_root_info(root_dir)
    imageInfo = np.load(os.path.join(root_dir, "CTinfo.npz"), allow_pickle=True)["info"]
    imageInfo = remove_duplicate(imageInfo)

    # for info in imageInfo:
    #     imgshape = np.load(info["imagePath"])["image"].shape
    #     thickness, spacing = info["sliceThickness"], info["pixelSpacing"]
    #     pstr = info["pstr"]
    #     dstr = info["date"]
    #     existId = (pos_df["patient"] == pstr) & (pos_df["date"] == dstr)
    #     pos = pos_df[existId][["x", "y", "z", "d"]].values
    #
    #     pos[:, 2] = pos[:, 2] - 1
    #     pos = np.array([resample_pos(p, thickness, spacing, imgshape=imgshape) for p in pos])
    drop_rows = []
    for i in tqdm(range(len(pos_df))):
        pos_info = pos_df.iloc[i]
        pstr = pos_info["patient"]
        dstr = pos_info["date"]
        ids = [id for id in range(len(imageInfo)) if imageInfo[id]["pstr"] == pstr and imageInfo[id]["date"] == dstr]
        assert len(ids) <= 1, "{:} \n {:}".format(ids, pos_info)
        if len(ids) == 0:
            print(pos_info["patient"])
            drop_rows.append(pos_df.index[i])
            continue
        imageId = ids[0]
        imagePath = imageInfo[imageId]["imagePath"]
        thickness = imageInfo[imageId]["sliceThickness"]
        spacing = imageInfo[imageId]["pixelSpacing"]
        # resolution = np.array(list(map(float, ([thickness] + list(spacing)))))
        # new_resolution = np.array([1, 1, 1])
        raw_pos = pos_info[["x", "y", "z", "d"]].values.astype(np.float)
        raw_pos[2] = raw_pos[2] - 1
        image = np.load(imagePath, allow_pickle=True)["image"]
        norm_pos = resample_pos(raw_pos, thickness, spacing, new_spacing=[1, 1, 1], imgshape=image.shape)
        pos_df.loc[pos_df.index[i], ["x", "y", "z", "d"]] = norm_pos
    pos_df = pos_df.drop(index=drop_rows)
    pos_df.to_csv(os.path.join(save_dir, "pos_labels_norm.csv"), index=False)
    print("save to {:s}".format(os.path.join(save_dir, "pos_labels_norm.csv")))




def get_infos_from_npz(root_dir):
    file = os.path.join(root_dir, "CTinfo.npz")
    # infos = np.load(file, allow_pickle=True)["info"]
    if os.path.exists(file):
        import shutil
        shutil.move(file, os.path.join(root_dir, "CTinfo_old.npz"))

    new_infos = []
    all_patients = [i for i in os.listdir(root_dir) if
                    os.path.isdir(os.path.join(root_dir, i)) and i[:4] == "Lung"]
    all_patients = natsorted(all_patients)
    for i in range(len(all_patients)):
        patientFolder = os.path.join(root_dir, all_patients[i])
        npz_files = [i for i in os.listdir(patientFolder) if i.endswith("_clean.npz")]
        if len(npz_files) == 0:
            npz_files = [i for i in os.listdir(patientFolder) if i.endswith("npz")]
        for nf in npz_files:
            info = np.load(os.path.join(patientFolder, nf), allow_pickle=True)["info"].tolist()
            new_infos.append(info)

    CTinfoPath = os.path.join(root_dir, "CTinfo.npz")
    np.savez_compressed(CTinfoPath, info=new_infos)
    print("Save all scan infos to {:s}".format(CTinfoPath))
    print("length is: ", len(new_infos))
    [print(i) for i in new_infos]

def extract_central_slice(root_dir, save_dir, ck_path, normalize=True):
    from PIL import Image

    checklist_df = pd.read_excel(ck_path, skiprows=1, dtype={"date": str})
    change_root_info(root_dir) # change image path in the root info file
    imageInfo = np.load(os.path.join(root_dir, "CTinfo.npz"), allow_pickle=True)["info"]
    imageInfo = remove_duplicate(imageInfo)
    norm_str = "norm" if normalize else "raw"
    save_dir = os.path.join(save_dir, "central_slices_{:s}".format(norm_str))
    os.makedirs(save_dir, exist_ok=True)

    for i in tqdm(range(len(imageInfo))):
        info = imageInfo[i]
        save_name = "{:s}_{:s}_{:s}".format(info["pstr"], info["patientID"], info["date"])
        save_name += "_PET" if info["PET"] == "Y" else ""

        imgPath, thickness, spacing = info["imagePath"], info["sliceThickness"], info["pixelSpacing"]
        pstr = info["pstr"]
        dstr = info["date"]
        img = np.load(imgPath)["image"]
        img = lumTrans(img)
        existId = (checklist_df["Patient\n Index"] == pstr) & (checklist_df["date"] == dstr)
        zs = checklist_df[existId]["z"].values
        for z in zs:
            z = z - 1
            if normalize:
                # Change position z from raw to normalized
                raw_pos = np.concatenate([[1, 1], [z], [1]])
                norm_pos = resample_pos(raw_pos, thickness, spacing, new_spacing=[1, 1, 1], imgshape=img.shape)
                z = int(norm_pos[2])
            c_img = img[z]
            image_save_dir = os.path.join(save_dir, save_name + "_no{:d}_z{:d}.png".format(i, z))
            PILimg = Image.fromarray(c_img)
            PILimg.save(image_save_dir)

def create_gt_csv(root_dir, save_dir, annot_file, normalize=True):
    '''
    create the ground truth csv file based on the annotation information.
    :param root_dir: root folder of the data
    :param save_dir: directory to save the pos_label.csv
    :param annot_file: makesense exported annotation csv file
    :return: None
    '''
    annotations = pd.read_csv(os.path.join(save_dir, annot_file),
                              names=["label", "x", "y", "w", "h", "imgName", "W", "H"])
    norm_str = "norm" if normalize else "raw"
    labels = []
    for i in range(len(annotations)):
        patient_obj = annotations.iloc[i]
        annot_list = patient_obj["imgName"].split("_")
        if len(annot_list) == 5:
            pstr, mrn, dstr, nodule_idx, z = annot_list
        else:
            pstr, mrn, dstr, pet, nodule_idx, z = annot_list
        # imgName_list = annot_list[2].split(".")
        # Series = imgName_list[1]
        # z, Z = [int(j) for j in imgName_list[-2][4:].split("_")]
        Series = "Unknown"
        z = z.strip("z.png")
        x = patient_obj["x"] + patient_obj["w"] / 2
        y = patient_obj["y"] + patient_obj["h"] / 2
        # d = np.sqrt(np.power(patient_obj["w"], 2) + np.power(patient_obj["h"], 2))
        d = np.max([patient_obj["w"], patient_obj["h"]])
        labels.append((pstr, dstr, Series, x, y, z, d))
    columns = ["patient", "date", "series", "x", "y", "z", "d"]
    label_df = pd.DataFrame(labels, columns=columns)
    label_df.to_csv(os.path.join(save_dir, "pos_labels_{:s}.csv".format(norm_str)), index=False)

    if not normalize:
        gt_labels_raw2normalized(root_dir, save_dir)

def update_dataset_details(root_dir, save_dir):
    import time
    # data_folder = os.path.join(root_dir, "Data")
    detail_path = os.path.join(save_dir, "details.xlsx")
    detail_df = pd.read_excel(detail_path, dtype={"MRN": str})
    detail_df['MRN'] = detail_df['MRN'].apply(lambda x: '{0:0>9}'.format(x))
    # gt_df = pd.read_csv(os.path.join(root_dir, "gt_labels.csv"))
    pos_df = pd.read_csv(os.path.join(root_dir, "pos_labels_norm.csv"), dtype={"MRN": str, "date": str})

    info_path = os.path.join(root_dir, "CTinfo.npz")
    infos = np.load(info_path, allow_pickle=True)["info"]

    # new_infos = []
    # duplicate = []
    # for info in infos:
    #     if info not in new_infos:
    #         new_infos.append(info)
    #     else:
    #         duplicate.append(info)
    # np.savez_compressed(info_path, info=new_infos)

    today = time.strftime('%Y%m%d', time.localtime())

    for info in tqdm(infos):
        pstr = info["pstr"]
        dstr = info["date"]
        patient_colname = "patient" if "patient" in pos_df.columns else 'Patient\n Index'
        assert patient_colname in pos_df
        existId = (pos_df[patient_colname] == pstr) & (pos_df["date"] == dstr)
        pos = pos_df[existId][["x", "y", "z", "d"]].values
        detailExistId = (detail_df["Patient Idx"] == pstr)
        if len(pos) > 0:
            annotated = detail_df.loc[detailExistId, "Annotated nodules\n(Date: number)"]
            if annotated.isnull().values.any():
                annotated = "{{{:s}:{:d}}}".format(dstr, len(pos))
            else:
                annotated = annotated + ", {{{:s}:{:d}}}".format(dstr, len(pos))
            detail_df.loc[detailExistId, "Annotated nodules\n(Date: number)"] = annotated
            inData = detail_df.loc[detailExistId, "Has annotation"]
            if inData.isnull().values.any():
                inData = 1
                detail_df.loc[detailExistId, "Has annotation"] = inData
        else:
            notAnnotated = detail_df.loc[detailExistId, "Not annotated nodules\n(Date)"]
            if notAnnotated.isnull().values.any():
                notAnnotated = "{:s}".format(dstr)
            else:
                notAnnotated = notAnnotated + ", {:s}".format(dstr)
            detail_df.loc[detailExistId, "Not annotated nodules\n(Date)"] = notAnnotated

        if detail_df.loc[detailExistId, "Data downloaded"].isnull().values.any():
            detail_df.loc[detailExistId, "Data downloaded"] = 1
        detail_df.loc[detailExistId, "Included date"] = today

    detail_df.to_excel(os.path.join(save_dir, "details_new.xlsx"), index=False)
    print("Saved to {:s}".format(os.path.join(save_dir, "details_new.xlsx")))

def update_dataset_checklist(root_dir, save_dir, ck_path):
    import time
    import itertools
    # data_folder = os.path.join(root_dir, "Data")
    # detail_path = os.path.join(save_dir, "details.xlsx")

    checklist_df = pd.read_excel(ck_path, skiprows=1, dtype={"date": str})
    # change_root_info(root_dir) # change image path in the root info file

    assign_PET_label(root_dir)
    # imageInfo = remove_duplicate(imageInfo)
    # norm_str = "norm" if normalize else "raw"
    # save_dir = os.path.join(save_dir, "central_slices_{:s}".format(norm_str))
    # os.makedirs(save_dir, exist_ok=True)


    # gt_df = pd.read_csv(os.path.join(root_dir, "gt_labels.csv"))
    try:
        pos_df = pd.read_csv(os.path.join(root_dir, "pos_labels_raw.csv"), dtype={"MRN": str, "date": str})
    except:
        gt_labels_normalized2raw(root_dir, root_dir)
        pos_df = pd.read_csv(os.path.join(root_dir, "pos_labels_raw.csv"), dtype={"MRN": str, "date": str})

    info_path = os.path.join(root_dir, "CTinfo.npz")
    infos = np.load(info_path, allow_pickle=True)["info"]


    today = time.strftime('%Y%m%d', time.localtime())

    for info in tqdm(infos):
        pstr = info["pstr"]
        dstr = info["date"]
        patient_colname = "patient" if "patient" in pos_df.columns else 'Patient\n Index'
        assert patient_colname in pos_df
        existId = (pos_df[patient_colname] == pstr) & (pos_df["date"] == dstr)
        pos = pos_df[existId][["x", "y", "z", "d"]].values
        checklistExistId = (checklist_df["Patient\n Index"] == pstr) & (checklist_df["date"] == dstr)

        if checklist_df[checklistExistId]['PET'].isnull().values.any():
            checklist_df.loc[checklistExistId, 'PET'] = info["PET"]
        if len(pos) > 0:
            annotated = checklist_df.loc[checklistExistId, ["x", "y", "z", "d"]]
            if annotated.isnull().values.any():
                for z in annotated["z"].unique():
                    found = pos[pos[:, 2] == z]
                    if len(found) > 0:
                        checklist_df.loc[checklistExistId & (annotated["z"] == z), ["x", "y", "z", "d"]] = pos[pos[:, 2] == z]
                        checklist_df.loc[checklistExistId & (annotated["z"] == z), "Included on"] = today

    pos_df = pd.read_csv(os.path.join(root_dir, "pos_labels_norm.csv"), dtype={"MRN": str, "date": str})
    for info in tqdm(infos):
        pstr = info["pstr"]
        paExistId = checklist_df["Patient\n Index"] == pstr
        all_dates = checklist_df.loc[paExistId, "date"]
        unique_dates = all_dates.value_counts().sort_values(ascending=False).index
        nodule_dict = {} # {0: (z0, d0, Y), 1: (z1, d1, N)}
        for dstr in unique_dates:

            patient_colname = "patient" if "patient" in pos_df.columns else 'Patient\n Index'
            assert patient_colname in pos_df
            existId = (pos_df[patient_colname] == pstr) & (pos_df["date"] == dstr)
            zs = pos_df[existId]['z'].values
            ds = pos_df[existId]['d'].values

            checklistExistId = (checklist_df["z"].notnull()) & (checklist_df["d"].notnull()) & \
                               paExistId & (checklist_df["date"] == dstr)
            checklist_lines = checklist_df[checklistExistId]
            if len(checklist_lines) != len(zs):
                print("{:d} samples in checklist, {:d} samples in pos_label, for pstr: {:s}, dstr: {:s}".format(
                    len(checklist_lines), len(zs), pstr, dstr))
                continue
            # zs = checklist_lines['z'].values
            # ds = checklist_lines['d'].values
            # first date, empty nodule_dict
            if len(checklist_lines) > 0:
                if len(nodule_dict) == 0:
                    peusdo_main_nodule = np.repeat("N", len(checklist_lines))
                    peusdo_main_nodule[np.argmax(ds)] = "Y"
                    for i in np.arange(len(checklist_lines)):
                        nodule_dict[i] = (zs[i], ds[i], peusdo_main_nodule[i])
                    if checklist_lines['nodule\nIndex'].isnull().values.any():
                        checklist_df.loc[checklistExistId, 'nodule\nIndex'] = np.arange(len(checklist_lines))
                    main_nodule = checklist_lines['main\n nodule']
                    if main_nodule.isnull().values.any() or (main_nodule == "U").values.all():
                        checklist_df.loc[checklistExistId, 'main\n nodule'] = peusdo_main_nodule
                else:
                    if checklist_lines['nodule\nIndex'].isnull().values.any() or \
                            main_nodule.isnull().values.any() or (main_nodule == "U").values.all():
                        candidates = np.stack([zs, ds], axis=0)
                        match_error = np.zeros([len(nodule_dict), len(zs)])
                        for i in nodule_dict:
                            match_error[i] = np.sum(np.abs(np.array(nodule_dict[i][:2])[:, np.newaxis] - candidates), axis=0)
                        options = np.array(list(itertools.permutations(range(len(nodule_dict)), len(zs))))
                        ids = np.repeat(np.arange(len(zs))[np.newaxis, :], len(options), axis=0)
                        errors = np.sum(match_error[options, ids], axis=1)
                        nodule_index = options[np.argmin(errors)]
                    if checklist_lines['nodule\nIndex'].isnull().values.any():
                        checklist_df.loc[checklistExistId, 'nodule\nIndex'] = nodule_index
                    if main_nodule.isnull().values.any() or (main_nodule == "U").values.all():
                        peusdo_main_nodule = np.array([nodule_dict[i][-1] for i in nodule_index])
                        checklist_df.loc[checklistExistId, 'main\n nodule'] = peusdo_main_nodule

    checklist_df.to_excel(os.path.join(save_dir, ck_path.replace(".xlsx", "_new.xlsx")), index=False)
    print("Saved to {:s}".format(os.path.join(save_dir, ck_path.replace(".xlsx", "_new.xlsx"))))


if __name__ == '__main__':

    # list_float_parser = lambda x: [float(i) for i in x.strip("[]").split(",")] if x else []
    parser = argparse.ArgumentParser(description="prepare script")
    parser.add_argument('command', choices=["prep_luna", "prep_methodist", "ch_infopath", "ass_pet", "get_info",
                                            "raw2norm", "check_label", "extract", "convert", "update_details", "update_checklist"],
                        help="options: [prep_luna, prep_methodist, ch_infopath, ass_pet, get_info, raw2norm, "
                             "check_label, extract, convert, update]", default="prep_methodist")
    parser.add_argument('-s', '--save_dir', type=str, help='save directory', default=None)
    parser.add_argument('-r', '--root_dir', type=str, help='root directory', default=None)
    parser.add_argument('-p', '--ck_path', type=str, help='checklist path', default=None)
    parser.add_argument('-a', '--annot_file', type=str, help='annotation file name', default="")
    parser.add_argument('-m', '--mask', type=eval, help='only apply mask in preprocessing', default=True)
    parser.add_argument('-c', '--crop', type=eval, help='crop masked images in preprocessing', default=True)
    parser.add_argument('-n', '--normalize', type=eval, help='normalized or raw data', default=True)
    args = parser.parse_args()

    root_dir = args.root_dir
    save_dir = args.save_dir
    ck_path = args.ck_path
    normalize = args.normalize
    annot_file = args.annot_file

    if args.command == "prep_luna":
        preprocess_luna()
    elif args.command == "prep_methodist":
        if args.mask:
            if args.crop:
                prepare_masked_cropped_images(root_dir, save_dir)
            else:
                prepare_masked_images(root_dir, save_dir)
    elif args.command == "ch_infopath":
        change_root_info(save_dir)
    elif args.command == "ass_pet":
        assign_PET_label(save_dir)
    elif args.command == "get_info":
        get_infos_from_npz(root_dir)
    elif args.command == "raw2norm":
        gt_labels_raw2normalized(root_dir, save_dir)
    elif args.command == "check_label":
        check_label_existance(root_dir, save_dir)
    elif args.command == "extract":
        extract_central_slice(root_dir, save_dir, ck_path, normalize)
    elif args.command == "convert":
        create_gt_csv(root_dir, save_dir, annot_file, normalize)
    elif args.command == "update_details":
        update_dataset_details(root_dir, save_dir)
    elif args.command == "update_checklist":
        update_dataset_checklist(root_dir, save_dir, ck_path)
